{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # importing OS in order to make GPU visible\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# specify which device you want to work on.\n",
    "# Use \"-1\" to work on a CPU. Default value \"0\" stands for the 1st GPU that will be used\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import sys # importyng sys in order to access scripts located in a different folder\n",
    "\n",
    "path2scripts = r'C:\\Users\\bcper\\Documents\\GitHub\\models\\research\\\\' \n",
    "sys.path.insert(0, path2scripts) # making scripts in models/research available for import\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code structure below courtesy https://github.com/nicknochnack/TFODCourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'resnet50' \n",
    "MODEL_VERSION = 'v1'\n",
    "PRETRAINED_MODEL_NAME = 'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz'\n",
    "CLASS_LABELS_FILE = 'Class Labels.txt'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "TFR_FILENAME = 'cleaned'\n",
    "EVAL_RECORD = 'xview_eval_cleaned.record'\n",
    "TRAIN_RECORD = 'xview_train_cleaned.record'\n",
    "TEST_RECORD = 'xview_test_cleaned.record'\n",
    "GEOJSON_FILE = 'train_data_clean.geojson'\n",
    "\n",
    "XVIEW_UTILS_PATH = r'C:\\Users\\bcper\\Documents\\GitHub'\n",
    "TF_INSTALL_PATH = r'C:\\Users\\bcper\\Documents\\GitHub\\models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: TFR_WORK_PATH, \n",
    "DATA_DIR_PATH = os.path.join(TF_INSTALL_PATH, 'workspace','data', '')\n",
    "\n",
    "paths = {\n",
    "    'WORK_PATH': os.path.join(TF_INSTALL_PATH, 'workspace'),\n",
    "    'TFR_WORK_PATH': os.path.join(XVIEW_UTILS_PATH,'xView_data_utilities'),\n",
    "    'SCRIPTS_PATH': os.path.join(TF_INSTALL_PATH,'research', 'object_detection'),\n",
    "    'LABELMAP_PATH': DATA_DIR_PATH,\n",
    "    'TFR_TRAIN_IMAGE': os.path.join(DATA_DIR_PATH, 'train_images',''),\n",
    "    'TFR_TEST_IMAGE': os.path.join(DATA_DIR_PATH, 'test_images', ''),\n",
    "    'MODEL_PATH': os.path.join(TF_INSTALL_PATH, 'workspace','models', MODEL_NAME, MODEL_VERSION, ''),\n",
    "    'PRETRAINED_MODEL': os.path.join(TF_INSTALL_PATH, 'workspace','pre-trained_models'),\n",
    "    'CHECKPOINT_PATH': os.path.join(TF_INSTALL_PATH, 'workspace','models', MODEL_NAME), \n",
    "    'EXPORT_PATH': os.path.join(TF_INSTALL_PATH, 'workspace','exported_models',MODEL_NAME + '_' + MODEL_VERSION),\n",
    "    'PROTOC_PATH':os.path.join(TF_INSTALL_PATH,'protoc'),\n",
    "    'GEOJSON_PATH': os.path.join(TF_INSTALL_PATH, 'workspace','data'),\n",
    "    'CLASS_LABEL_PATH': DATA_DIR_PATH,\n",
    "    'DATA_PATH': DATA_DIR_PATH\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'PIPELINE_CONFIG_SOURCE': os.path.join(paths['PRETRAINED_MODEL'],PRETRAINED_MODEL_NAME, 'pipeline.config'), \n",
    "    'PIPELINE_CONFIG':os.path.join(TF_INSTALL_PATH, 'workspace','models', MODEL_NAME, MODEL_VERSION, 'pipeline.config'),\n",
    "    'LABELMAP': os.path.join(paths['LABELMAP_PATH'], LABEL_MAP_NAME),\n",
    "    'GEOJSON': os.path.join(paths['GEOJSON_PATH'], GEOJSON_FILE),\n",
    "    'CLASS_LABELS': os.path.join(paths['CLASS_LABEL_PATH'], CLASS_LABELS_FILE),\n",
    "    'TRAIN_RECORD': os.path.join(paths['DATA_PATH'], TRAIN_RECORD),\n",
    "    'EVAL_RECORD': os.path.join(paths['DATA_PATH'], EVAL_RECORD),\n",
    "    'TEST_RECORD': os.path.join(paths['DATA_PATH'], TEST_RECORD),\n",
    "    'FINE_TUNE_CKPT': os.path.join(paths['PRETRAINED_MODEL'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0'),\n",
    "    'TRAINING_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], 'model_main_tf2.py')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'CLASSES': 95,\n",
    "    'STEPS': 10000,\n",
    "    'BATCH_SIZE': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the TF .record files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(paths['TFR_WORK_PATH'])\n",
    "test_ratio = .3\n",
    "TFR_TEST_FILENAME = TFR_FILENAME + 'test'\n",
    "\n",
    "\n",
    "generate_train_and_eval = 'process_wv.py {} {} -s={} -t={}'.format(paths['TFR_TRAIN_IMAGE'], files['GEOJSON'], TFR_FILENAME, test_ratio)\n",
    "%run {generate_train_and_eval}\n",
    "\n",
    "generate_test = 'process_wv.py {} {} -s={} -t={}'.format(paths['TFR_TEST_IMAGE'], files['GEOJSON'], TFR_FILENAME, 0)\n",
    "%run {generate_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(paths['DATA_PATH'])\n",
    "#read in the class labels\n",
    "class_dict = {}\n",
    "file = open(files['CLASS_LABELS'], \"r\")\n",
    "\n",
    "#create a class label dictionary\n",
    "for line in file:\n",
    "    key, value = line.split(':')\n",
    "    class_dict[int(key)] = value.strip()\n",
    "\n",
    "\n",
    "#this will make the list in the correct label format for tensorflow\n",
    "x = ''\n",
    "for key in class_dict:\n",
    "    x = x + ('item { \\n\\tid: ' + str(key) + '\\n\\tname: \\\"' + class_dict.get(key) + '\\\"\\n}\\n\\n')\n",
    "    \n",
    "# Write the label map to a file\n",
    "lmap = open(files['LABELMAP'],\"w\")#write mode\n",
    "lmap.write(x)\n",
    "lmap.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the .config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "config_file = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG_SOURCE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG_SOURCE'], \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)\n",
    "\n",
    "    \n",
    "#this is specific to the faster_rcnn model, for others we would need to change this\n",
    "pipeline_config.model.faster_rcnn .num_classes = config['CLASSES']\n",
    "\n",
    "\n",
    "\n",
    "pipeline_config.train_config.batch_size = config['BATCH_SIZE']\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(files['FINE_TUNE_CKPT'])\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_config.num_steps = config['STEPS']\n",
    "pipeline_config.train_input_reader.label_map_path= label_map_path\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [files['TRAIN_RECORD']]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [files['EVAL_RECORD']]\n",
    "\n",
    "\n",
    "config_text = text_format.MessageToString(pipeline_config)    \n",
    "\n",
    "#create the folder if if doesnt exist\n",
    "if not os.path.exists(paths['MODEL_PATH']):\n",
    "    os.makedirs(paths['MODEL_PATH'])\n",
    "   \n",
    "    \n",
    "if os.path.exists(files['PIPELINE_CONFIG']):\n",
    "    with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n",
    "        f.write(config_text)\n",
    "else:\n",
    "    open(paths['MODEL_PATH'] + \"pipeline.config\", \"x\")\n",
    "    \n",
    "    with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n",
    "        f.write(config_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(paths['SCRIPTS_PATH'])\n",
    "#run the model training\n",
    "\n",
    "train_model = '{} --pipeline_config_path={} --model_dir={}  --alsologtostderr'.format('model_main_tf2.py', files['PIPELINE_CONFIG'], paths['MODEL_PATH'])\n",
    "%run {train_model}\n",
    "\n",
    "print(train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(paths['SCRIPTS_PATH'])\n",
    "#run the model evaluation\n",
    "\n",
    "evaluate_model = '{} --pipeline_config_path={} --model_dir={} --checkpoint_dir={}  --sample_1_of_n_eval_examples=1'.format('model_main_tf2.py', files['PIPELINE_CONFIG'], paths['MODEL_PATH'], paths['MODEL_PATH'])\n",
    "%run {evaluate_model}\n",
    "\n",
    "print(evaluate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "tensorboard --logdir=C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\models\\resnet50\n"
     ]
    }
   ],
   "source": [
    "os.chdir(paths['WORK_PATH'])\n",
    "tensorboard_path = paths['CHECKPOINT_PATH']\n",
    "\n",
    "!tensorboard --logdir=$tensorboard_path\n",
    "\n",
    "print(tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bcper\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0608 15:14:49.762000  3396 deprecation.py:596] From C:\\Users\\bcper\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0608 15:14:57.885536  3396 convolutional_keras_box_predictor.py:153] depth of additional conv before box predictor: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bcper\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:464: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0608 15:15:04.128157  3396 deprecation.py:330] From C:\\Users\\bcper\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:464: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x00000269481416A0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0608 15:15:35.597015  3396 save_impl.py:76] Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x00000269481416A0>, because it is not built.\n",
      "W0608 15:16:04.215264  3396 save.py:238] Found untraced functions such as FirstStageBoxPredictor_layer_call_fn, FirstStageBoxPredictor_layer_call_and_return_conditional_losses, mask_rcnn_keras_box_predictor_layer_call_fn, mask_rcnn_keras_box_predictor_layer_call_and_return_conditional_losses, FirstStageBoxPredictor_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n",
      "C:\\Users\\bcper\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0608 15:16:20.134264  3396 save.py:1239] FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\exported_models\\resnet50_v1\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0608 15:16:21.990837  3396 builder_impl.py:774] Assets written to: C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\exported_models\\resnet50_v1\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing pipeline config file to C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\exported_models\\resnet50_v1\\pipeline.config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0608 15:16:29.082533  3396 config_util.py:253] Writing pipeline config file to C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\exported_models\\resnet50_v1\\pipeline.config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporter_main_v2.py --pipeline_config_path=C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\models\\resnet50\\v1\\pipeline.config --trained_checkpoint_dir=C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\models\\resnet50\\v1\\ --output_directory=C:\\Users\\bcper\\Documents\\GitHub\\models\\workspace\\exported_models\\resnet50_v1 --input_type=image_tensor\n"
     ]
    }
   ],
   "source": [
    "os.chdir(paths['SCRIPTS_PATH'])\n",
    "\n",
    "export_model = '{} --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={} --input_type=image_tensor'.format('exporter_main_v2.py', files['PIPELINE_CONFIG'], paths['MODEL_PATH'], paths['EXPORT_PATH'])\n",
    "%run {export_model}\n",
    "\n",
    "print(export_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "neptune": {
   "notebookId": "7c618cd5-39ec-46c6-bee7-0cfe5297f22a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
